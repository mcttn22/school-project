ğ—”ğ—¯ğ—¼ğ˜‚ğ˜ ğ—”ğ—œ:
â€¢ Artificial Intelligence (AI) mimics human cognition in order to perform tasks and learn from them
â€¢ Machine Learning is a subfield of AI that uses algorithms trained on data to produce adaptable models (trained programs)
â€¢ Deep Learning is a subfield of Machine Learning that uses Artificial Neural Networks (ANNs)
â€¢ The focus of this project will be on learning ANNs (creating a simple ANN from scratch) and their applications (image recognition)

ğ™ƒğ™¤ğ™¬ ğ˜¼ğ™§ğ™©ğ™ğ™›ğ™ğ™˜ğ™ğ™–ğ™¡ ğ™‰ğ™šğ™ªğ™§ğ™–ğ™¡ ğ™‰ğ™šğ™©ğ™¬ğ™¤ğ™§ğ™ ğ™¨ ğ™¬ğ™¤ğ™§ğ™ :
ğ™ğ™©ğ™§ğ™ªğ™˜ğ™©ğ™ªğ™§ğ™š ğ™¤ğ™› ğ™–ğ™£ ğ˜¼ğ™‰ğ™‰:
â€¢ Input layer of multiple inputs (in an array)
â€¢ Hidden layers of calculations (the more layers, the more accurate the prediction), consisting of the following:
    â€“ An Activation function:
        âˆ— Calculate the dot product of the input array with a hidden weight array, then sum the result with a bias
    â€“ A Transfer function to get an output, Eg: Sigmoid function to transform the result of the Activation function to a number between 0 and 1,
    then the result can be classified as closer to 0 or 1 (known as logistic regression),with 0 being one state and 1 being another
â€¢ Output layer to output the final result of the calculations from the hidden layers, consisting of the following:
    â€“ An Activation function:
        âˆ— Caclulate the dot product of the hidden layer output with an output weight array, then sum the result with a bias
    â€“ A Transfer function to get an output (round output to one end of range with a meaning)

ğ™ƒğ™¤ğ™¬ ğ˜¼ğ™‰ğ™‰ğ™¨ ğ™–ğ™§ğ™š ğ™©ğ™§ğ™–ğ™ğ™£ğ™šğ™™:
â€¢ Forward Propagation, the process of feeding inputs into the neural network and getting a result/prediction
â€¢ Back Propagation, the process of calculating the error in the prediction then adjusting the weights and biases accordingly, consisting of the following:
    â€“ A Cost function (used for graphs and deriving formula for gradient descent):
        âˆ— Finds the average of the difference between the actual and the predicted value for each input
    â€“ Gradient descent:
        âˆ— Update the hidden/output weight arrays and bias, by subtracting the rate of change of Cost with respect to Weight, multiplied with a learning rate
        âˆ— This repetitive process will continue to reduce the cost to a minimum, if the learning rate is set to an appropriate value